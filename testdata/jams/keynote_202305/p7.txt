其实在GPU行业内，游戏技术是衡量技术创新能力的重要标志，
由此发展起来的GPU计算，推动了整个人工智能产业的创新，而开发者也推动了GPU的应用创新。
下面，我们进入发布会的第5部分，MUSA。
熟悉我们的朋友知道，去年我们发布了MUSA软件栈。
经过一年多的产品测试及调优，我们将正式开启【MUSA开发者生态计划】。
摩尔线程将全力为合作伙伴与开发者提供全套的MUSA开发工具、编程指南、系列教程和开源的框架及模型库等。
那我们同时也会联合业内第三方社区共同建设 MUSA的开发者生态，和各类开发者一起，举办软硬件共创活动，一起开发新的算法模型、计算系统和平台，让社区的价值不断的体现出来。 
我们首先会在社区中开源MUSA toolkit 1.0软件工具包，包含底层的驱动、编译器、AI加速库、数学库、通信库等等，让用户快速适配并充分发挥摩尔线程GPU的计算能力。
最方便的是，MUSA可以从运行时和数学库API的层面，都能够兼容CUDA。
而且我们提供CUDA转MUSA自动移植工具musify，对于许多AI开发者原有的算法，迁移起来会非常的便捷。
那么使用MUSA时候到底有多便捷呢？下面我们举两个例子：
第一个例子呢，使用MUSA加速生命科学领域的典型应用——冷冻电镜三维重建：
过去一年多呢，摩尔线程与清华大学蛋白质研究技术中心的李雪明教授团队密切合作，先后在两代三维电镜的重构算法上，分别基于Open CL和MUSA，进行了国产化平台的开发和移植适配。
比起Open CL适配，使用摩尔线程MUSA自动化移植工具，可以快速地完成CUDA算法的迁移，迁移适配时间由14周降至2周，研究人员可以更好的聚焦在算法的性能调优上。
我们再来看一个借助MUSA来实现AI算法模型平滑迁移的例子：
我们以AIGC内容生成模型Stable Diffusion为例，模型从网络结构的实现上需要49个算子，除去DNN库已经实现的算子外，还需要新开发7个算子，
如果用Open CL开发，总需21人天。
而如果使用MUSA，开发者可以通过musify自动转换工具，直接移植CUDA源码，大约在2人天内便可以完成快速适配和迁移，整体上能节省90%以上的时间，非常高效。
相信大家已经看到了MUSA的强大之处。
现阶段呢，很多AI应用都是基于开源框架Pytorch完成的。
如何让MUSA来帮助这些用户实现快速的适配呢？
我们将在MUSA社区正式推出开源的MT Pytorch：
那么，基于MUSA，用户可以复用PyTorch开源社区的大量模型算子，降低开发成本；
并且MT Pytorch可以很好地支持多种模型的推理，覆盖CV、NLP、语音等多个领域；
能够运行ChatGLM，Stable Diffusion，LLaMA等典型的大模型的分布式多卡推理；
另外我们也可以支持单机多卡与多机多卡的分布式训练，
利用数据并行、模型并行以及ZERO等分布式训练技术，我们可以完成简单基础模型以及典型Transformer结构的NLP语言模型的训练；
那开发者如何更好的完成模型适配呢？
我们为社区带来了摩尔线程的MUSA预训练模型，MUSA Bert和MUSA Sim。
MUSA Bert是摩尔线程自研的语言理解大模型，以较小的规模就取得了很高的性能。
使用了高效的软硬一体分布式训练策略，优化的数据IO管线、高质量的数据集、和领域数据增强，结合对比学习、对抗学习、高性能优化器等技术，提高了模型的数据和参数利用的效率。
当前，我们训练的Musa Bert模型，仅以3亿模型参数规模，就跻身中文语言理解测评基准CLUE排行榜前十名，是前十名中最小的单一模型。
我们的Musa Sim模型更是取得了语义相似度单一任务榜单首位。
此外，我们的通用模型在阅读理解、声韵识别、智能客服等多个领域，也取得了具有竞争力的性能。
近几个月呢，AI大语言模型的浪潮可以说已经席卷全球。
尤其是ChatGPT迅速出圈，各行各业都为之疯狂。
摩尔线程其实很早就开始研发类chatGPT的大模型了，并提前进行了大模型训练、精调、压缩、推理的布局，
后续，我们陆续会在MUSA社区中，提供从六千万到三十亿参数的全套的MUSA系列模型，方便开发者适配和移植。